所有文章的集合 + 一句话介绍
https://www.paperdigest.org/2019/11/neurips-2019-highlights/

https://papers.nips.cc/paper/8477-controlling-neural-level-sets.pdf
控制level set, 有趣的方向

https://papers.nips.cc/paper/8509-initialization-of-relus-for-dynamical-isometry
ReLU dynamical isometry, 有限宽度，有趣

https://papers.nips.cc/paper/8498-more-is-less-learning-efficient-video-representations-by-big-little-network-and-depthwise-temporal-aggregation
动作识别网络，更小

https://papers.nips.cc/paper/8482-rethinking-generative-mode-coverage-a-pointwise-guaranteed-approach
GAN? is it interesting? 

https://papers.nips.cc/paper/8513-backprop-with-approximate-activations-for-memory-efficient-network-training
more Memory-efficient; 类似于 Transformer, 可逆的activation节省memory

https://papers.nips.cc/paper/8479-an-improved-analysis-of-training-over-parameterized-deep-neural-networks.pdf
Quanquan Gu的文章，稍微窄一点的网络

https://papers.nips.cc/paper/8518-total-least-squares-regression-in-input-sparsity-time
找sparse solution的时间, Woodroof的文章

https://papers.nips.cc/paper/8524-asymmetric-valleys-beyond-sharp-and-flat-local-minima
黄高的文章，不对称valley, 平和尖的Local-min

https://papers.nips.cc/paper/8529-stagewise-training-accelerates-convergence-of-testing-error-over-sgd
解释stagewise training, 杨天保

https://papers.nips.cc/paper/8532-on-learning-over-parameterized-neural-networks-a-functional-approximation-perspective
Su Lili的文章, n-neuron network的收敛性分析

https://papers.nips.cc/paper/8536-discovering-neural-wirings
发现wiring patterns; 比NAS更严格

https://papers.nips.cc/paper/8559-on-lazy-training-in-differentiable-programming
Lazy training, 经典文章 by Chizat, Bach

https://papers.nips.cc/paper/8560-quality-aware-generative-adversarial-networks
Quality-aware GAN 可以参考

https://papers.nips.cc/paper/8579-backpropagation-friendly-eigendecomposition
stable eigenvalues in deep learning

https://papers.nips.cc/paper/8583-implicit-regularization-of-discrete-gradient-dynamics-in-linear-neural-networks
Bach的文章，线性网络的收敛性

https://papers.nips.cc/paper/8598-communication-efficient-distributed-learning-via-lazily-aggregated-quantized-gradients
GG的文章, quantized gradient

https://papers.nips.cc/paper/8609-sgd-on-neural-networks-learns-functions-of-increasing-complexity
SGD学习越来越复杂的函数

https://papers.nips.cc/paper/8610-the-landscape-of-non-convex-empirical-risk-with-degenerate-population-risk
联系non-convex 的population和ERM landscape

https://papers.nips.cc/paper/8618-deconstructing-lottery-tickets-zeros-signs-and-the-supermask
lottery tick, and supermask

https://papers.nips.cc/paper/8624-tight-dimension-independent-lower-bound-on-the-expected-convergence-rate-for-diminishing-step-sizes-in-sgd
lower bound of SGD

https://papers.nips.cc/paper/8642-principal-component-projection-and-regression-in-nearly-linear-time-through-asymmetric-svrg
解PCP in linear time by SVRG 

https://papers.nips.cc/paper/8674-data-cleansing-for-models-trained-with-sgd
Data cleaning for models 

https://papers.nips.cc/paper/8689-understanding-and-improving-layer-normalization
understanding layer-normalization

https://papers.nips.cc/paper/8693-massively-scalable-sinkhorn-distances-via-the-nystrom-method
更快的计算sinkhorn距离的方法

https://papers.nips.cc/paper/8694-double-quantization-for-communication-efficient-distributed-optimization
量化, quantization SGD. 

https://papers.nips.cc/paper/8717-when-does-label-smoothing-help
什么时候label smoothing helps? 

https://papers.nips.cc/paper/8723-the-convergence-rate-of-neural-networks-for-learned-functions-of-different-frequencies
什么时候学习到不同的frequency

https://papers.nips.cc/paper/8737-tight-certificates-of-adversarial-robustness-for-randomly-smoothed-classifiers
Yuan Yang的文章, random smoothing的提升

https://papers.nips.cc/paper/8739-one-ticket-to-win-them-all-generalizing-lottery-ticket-initializations-across-datasets-and-optimizers
winning ticket对不同的网络都能成立

https://papers.nips.cc/paper/8741-fair-algorithms-for-clustering
fairnesss for clustering 一个经典问题

https://papers.nips.cc/paper/8742-learning-mean-field-games
学习mean-field games

https://papers.nips.cc/paper/8757-e2-train-training-state-of-the-art-cnns-with-over-80-less-energy
节省80%的训练时间--对ResNet, on-device learning


好玩的文章：
https://papers.nips.cc/paper/8768-face-reconstruction-from-voice-using-generative-adversarial-networks
根据声音重建人脸

https://papers.nips.cc/paper/8770-on-testing-for-biases-in-peer-review
讨论peer-review里的bias 

https://papers.nips.cc/paper/8775-input-similarity-from-the-neural-network-perspective
神经网络如何看待相似性

https://papers.nips.cc/paper/8777-weight-agnostic-neural-networks
只用结构不用weights, David Ha， 有一些关注
