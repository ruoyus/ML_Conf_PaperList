# Papers related to Adam

## Mechanisms

1.[A spherical analysis of Adam with batch normalization.](https://arxiv.org/pdf/2006.13382.pdf)  

Simon Roburin(École des Ponts, UPE), Yann de Mont-Marin(Département d’informatique de l’ENS, PSL Inria), Andrei Bursuc(valeo.ai), Renaud Marlet(valeo.ai), Patrick Pérez(valeo.ai), Mathieu Aubry(École des Ponts, UPE)

2.[Adam for Attention models](https://papers.nips.cc/paper/2020/file/b05b57f6add810d3b7490866d74c0053-Paper.pdf)

Jingzhao Zhang(MIT), Sai Praneeth Karimireddy(EPFL), Andreas Veit(Google Research), Seungyeon Kim(Google Research), Sashank Reddi(Google Research), Sanjiv Kumar(Google Research), Suvrit Sra(MIT)

Accepted by NIPS 2020

## New variants

1.[Adam+: A Stochastic Method with Adaptive Variance Reduction](https://arxiv.org/abs/2011.11985))

Mingrui Liu(Boston University), Wei Zhang(IBM Watson Research Center), Francesco Orabona(Boston University), Tianbao Yang(University of Iowa)

2.[Adaptive Gradient Method with Resilience and Momentum](https://arxiv.org/pdf/2010.11041.pdf)

Jie Liu, Chen Lin, Chuming Li, Ming Sun, Junjie Yan (Sensetime Research Group), Lu Sheng(Beihang University), Wanli Ouyang(The university of Sydney)

3.[AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients](https://arxiv.org/pdf/2010.07468.pdf)

Juntang Zhuang(Yale), Tommy Tang(UIUC), Yifan Ding(University of Central Florida), Sekhar Tatikonda(Yale), Nicha Dvornek(Yale), Xenophon Papademetris(Yale), James S. Duncan(Yale)

Accepted by NIPS 2020

4.[Stochastic Polyak Step-size for SGD: An Adaptive Learning Rate for Fast Convergence](https://arxiv.org/pdf/2002.10542.pdf)

Nicolas Loizou(Université de Montréal), Sharan Vaswani(Université de Montréal), Issam Laradji(University of British Columbia), Simon Lacoste-Julien(Université de Montréal).





