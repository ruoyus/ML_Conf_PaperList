# Selection of accepted papers

## [Click HERE for the list of papers with scores.](https://docs.google.com/spreadsheets/d/1n58O0lgGI5kI0QQY9f4BDDpNB4oFjb5D51yMr9fHAK4/edit#gid=1546418007)



Global Convergence of Three-layer Neural Networks in the Mean Field Regime https://openreview.net/forum?id=KvyxFqZS_D 7.5 [7.0, 7.0, 7.0, 9.0] Accept (Oral)

Orthogonalizing Convolutional Layers with the Cayley Transform https://openreview.net/forum?id=Pbj8H_jEHYv 7.25 [7.0, 7.0, 7.0, 8.0] Accept (Spotlight)

Minimum Width for Universal Approximation https://openreview.net/forum?id=O-XJwyoIF-k 7.25 [8.0, 7.0, 7.0, 7.0] Accept (Spotlight)

On the Origin of Implicit Regularization in Stochastic Gradient Descent https://openreview.net/forum?id=rq_Qr0c1Hyo 7.25 [7.0, 7.0, 7.0, 8.0] Accept (Poster)

Federated Learning Based on Dynamic Regularization https://openreview.net/forum?id=B7v4QMR6Z9w 7.25 [7.0, 8.0, 7.0, 7.0] Accept (Oral)

Growing Efficient Deep Networks by Structured Continuous Sparsification https://openreview.net/forum?id=wb3wxCObbRT 7.25 [7.0, 7.0, 7.0, 8.0] Accept (Oral)

Comments from Yite: It is a paper which modifies DARTS to let it work on structured pruning. Though I don't think it has a lot of novelty on relaxation of discrete structure optimization, there are several other things that make it stands out. For example, it uses sampling to reduce the training time of growing architecture. What's more, it also uses budget aware growing to make sparsity requirements satisfied.(It is actually like Augmented Lagrangian in spirit.) All in all, its performance is good compared to other recent methods. 

Benefit of deep learning with non-convex noisy gradient descent: Provable excess risk bound and superiority to kernel methods https://openreview.net/forum?id=2m0g1wEafh 7.25 [7.0, 8.0, 6.0, 8.0] Accept (Spotlight)

Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets? https://openreview.net/forum?id=uCY5MuAxcxU 7.25 [7.0, 7.0, 8.0, 7.0] Accept (Oral)

Is Attention Better Than Matrix Decomposition? https://openreview.net/forum?id=1FvkSpWosOl 7.25 [6.0, 8.0, 8.0, 7.0] Accept (Poster)

Sharpness-aware Minimization for Efficiently Improving Generalization https://openreview.net/forum?id=6Tm1mposlrM 7.25 [7.0, 8.0, 6.0, 8.0] Accept (Spotlight)

Rethinking Architecture Selection in Differentiable NAS https://openreview.net/forum?id=PKubaeJkw3 7.75 [7.0, 7.0, 10.0, 7.0] Accept (Oral)

continue...

A Gradient Flow Framework For Analyzing Network Pruning	https://openreview.net/forum?id=rumv7QmLUue	6.5	['7', '9', '5', '5'] Accept (Spotlight)

Comments from Yite: It is a very interesting paper to me. This paper use evolution of parameters under gradient flow to explain different pruning criterions including Magnitude pruning/SNIP/GraSP. What's more, it goes beyond observation to propose modification to existing pruning methods. The combination of theory/observation/modification is quite balanced to me.