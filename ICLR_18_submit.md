Deep Learning Theory (Geometry of Neural Network):
1. THE POWER OF DEEPER NETWORKS FOR EXPRESSING NATURAL FUNCTIONS
David Rolnick, Max Tegmark

2. ON THE INFORMATION BOTTLENECK THEORY OF DEEP LEARNING
Andrew M. Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan D. Tracey, David D. Cox

3. LEARNING ONE-HIDDEN-LAYER NEURAL NETWORKS WITH LANDSCAPE DESIGN
Rong Ge, Jason D. Lee, Tengyu Ma

4. EXPRESSIVE POWER OF RECURRENT NEURAL NET- WORKS
Valentin Khrulkov, Alexander Novikov, Ivan Oseledets

5. EXPONENTIALLY VANISHING SUB-OPTIMAL LOCAL MINIMA IN MULTILAYER NEURAL NETWORKS
Daniel Soudry, Elad Hoffer

6. GLOBAL OPTIMALITY CONDITIONS FOR DEEP NEURAL NETWORKS
Chulhee Yun, Suvrit Sra & Ali Jadbabaie

7. ON THE EXPRESSIVE POWER OF OVERLAPPING ARCHITECTURES OF DEEP LEARNING
Or Sharir & Amnon Shashua

8. UNDERSTANDING DEEP NEURAL NETWORKS WITH RECTIFIED LINEAR UNITS
Raman Arora, Amitabh Basu, Poorya Mianjy, Anirbit Mukherjee

Deep Learning Theory (First Order Methods for Neural Network):

9. NEGATIVE EIGENVALUES OF THE HESSIAN IN DEEP NEURAL NETWORKS 
Guillaume Alain, Nicolas Le Roux, Pierre-Antoine Manzagol

10. EASING NON-CONVEX OPTIMIZATION WITH NEURAL NETWORKS
David Lopez-Paz, Levent Sagun

11. STOCHASTIC GRADIENT DESCENT PERFORMS VARIATIONAL INFERENCE, CONVERGES TO LIMIT CYCLES FOR DEEP NETWORKS
Pratik Chaudhari, Stefano Soatto

12. TOWARDS BETTER UNDERSTANDING OF GRADIENT-BASED ATTRIBUTION METHODS FOR DEEP NEURAL NETWORKS
Marco Ancona, Enea Ceolini, Cengiz Öztireli, Markus Gross

13. BEYOND FINITE LAYER NEURAL NETWORKS: BRIDGING DEEP ARCHITECTURES AND NUMERICAL DIFFERENTIAL EQUATIONS
Yiping Lu, Aoxiao Zhong, Quanzheng Li, Bin Dong

14. FINDING FLATTER MINIMA WITH SGD
Stanisław Jastrze ̨bski,, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio & Amos Storkey

15. GRADIENTS EXPLODE
- DEEP NETWORKS ARE SHALLOW - RESNET EXPLAINED
George Philipp, Dawn Song, Jaime G. Carbonell

16. EMPIRICAL ANALYSIS OF THE HESSIAN OF OVER-PARAMETRIZED NEURAL NETWORKS
Levent Sagun, Utku Evci, V. Ug ̆ur Güney, Yann Dauphin, Léon Bottou

17. SGD LEARNS OVER-PARAMETERIZED NETWORKS THAT PROVABLY GENERALIZE ON LINEARLY SEPARABLE DATA
Alon Brutzkus & Amir Globerson, Eran Malach & Shai Shalev-Shwartz

Deep Learning Models & Algorithms:

18. ON THE CONVERGENCE OF ADAM AND BEYOND
Sashank J. Reddi, Satyen Kale & Sanjiv Kumar

19. ON THE INSUFFICIENCY OF EXISTING MOMENTUM SCHEMES FOR STOCHASTIC OPTIMIZATION
Rahul Kidambi1, Praneeth Netrapalli, Prateek Jain and Sham M. Kakade

20. WASSERSTEIN AUTO-ENCODERS
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, Bernhard Scho ̈lkopf

21. MATRIX CAPSULES WITH EM ROUTING
Geoffrey Hinton, Sara Sabour, Nicholas Frosst

22. TOWARDS DEEP LEARNING MODELS RESISTANT TO ADVERSARIAL ATTACKS
Aleksander Ma ̨dry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu

23. COMPOSITIONAL ATTENTION NETWORKS FOR MACHINE REASONING
Drew A. Hudson, Christopher D. Manning

24. DEEP COMPLEX NETWORKS
Chiheb Trabelsi, Olexa Bilaniuk, Ying Zhang, Dmitriy Serdyuk, Sandeep Subramanian, João Felipe Santos, Soroush Mehri, Negar Rostamzadeh, Yoshua Bengio & Christopher J Pal

25. DIVIDE AND CONQUER NETWORKS
Alex Nowak, David Folqué, Joan Bruna

GAN:
26. DO GANS LEARN THE DISTRIBUTION? SOME THEORY AND EMPIRICS
Sanjeev Arora, Andrej Risteski, Yi Zhang

27. MASKGAN: BETTER TEXT GENERATION VIA FILLING IN THE
William Fedus, Ian Goodfellow and Andrew M. Dai

28. SOBOLEV GAN
Youssef Mroueh, Chun-Liang Li, Tom Sercu, Anant Raj, & Yu Cheng

29. IMPROVING THE IMPROVED TRAINING OF WASSERSTEIN GANS:
A CONSISTENCY TERM AND ITS DUAL EFFECT
Xiang Wei, Boqing Gong, Zixia Liu, Wei Lu, Liqiang Wang

30. ON THE DISCRIMINATION-GENERALIZATION TRADE-OFF IN GANS
Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, Xiaodong He

31. ON THE REGULARIZATION OF WASSERSTEIN GANS
Henning Petzka, Asja Fischer & Denis Lukovnikov
